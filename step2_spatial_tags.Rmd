---
title: "Geographical analysis of media"
subtitle: "2. Geographical tags"
author: "Claude Grasland"
output: html_notebook
---


```{r setup2, echo = FALSE, comment = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = FALSE, warning = FALSE)
library(knitr)
library(dplyr)
library(quanteda)
library(stringr)
```




# Geographical tags

The aim of this section is to add to the quanteda corpus different metadata related to the geographical entities that are mentioned in the news. We do not discuss here the problems related to the choice of a list of entities and we just apply a method of recognition based on a dictionary. We distinguish between three categories of geographical entities : 

- states : recognized by a combination of country names or capital cities
- geographical regions : based on continents or other "natural" features like sea, topography, biogeography, ...
- international organizations ; based on a list established with wikipedia.

More details on the methodology for the creation of dictionaries are discussed in the media cookbook. 


## Preparation of data

### Load list and definition of entities

```{r load_entities}
#Load dictionary
ent<-readRDS("data/dict/worldgeo_def_V1.RDS")

# Select language of definition
ent <- ent[ent$lang=="fr",-1]

# Eliminate duplicated labels
ent <- ent[duplicated(ent)==F,]
ent <- ent[duplicated(ent$label)==F,]
ent <- ent[duplicated(ent$code)==F,]
# Visualize
head(ent)
```



### Load dictonary

We start by loading the last version of the Imageun dictionary and we extract our target language (here : french).

```{r load_dict}
#Load dictionary
dict<-readRDS("data/dict/worldgeo_dict_V4.RDS")

# Eliminate wikipedia codes
dict <- dict[dict$lang=="fr",-1]

# Eliminate duplicated labels
dict <- dict[duplicated(dict)==F,]
dict <- dict[duplicated(dict$label)==F,]

# Check if all codes are available
dict <- dict[dict$code %in% ent$code,]

# Visualize
head(dict)
```

### Load corpus

```{r}
qd <- readRDS("data/corpus/qd_mycorpus.RDS")
```

### Load tagging function

```{r func_annotate}
extract_tags <- function(qd = qd,                      # the corpus of interest
                         lang = "fr",                  # the language to be used
                         dict = dict,                  # the dictionary of target 
                         code = "id" ,                  # variable used for coding
                         tagsname = "tags",                 # name of the tags column
                         split  = c("'","’","-"),       # split list
                         tolow = FALSE  ,                # Tokenize text
                         comps = c("Afrique du sud")  # compounds
                         )
{ 


  
# Tokenize  
x<-as.character(qd)


if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       x <- gsub(reg," ",x)}  
if(tolow) { x <- tolower(x)} 
toks<-tokens(x)

# compounds
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       comps<- gsub(reg," ",comps)}  
if(tolow)       {comps <- tolower(comps)}  
toks<-tokens_compound(toks,pattern=phrase(comps))

  
# Load dictionaries and create compounds

  ## Target dictionary
dict<-dict[dict$lang==lang & is.na(dict$label)==F,]
target<-dict[ntoken(dict$label)>1,]
labels <-dict$label
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       labels<- gsub(reg," ",labels)}  
if(tolow)       {labels <- tolower(labels)}  
toks<-tokens_compound(toks,pattern=phrase(labels))
  
 # create quanteda dictionary
keys <-gsub(" ","_",labels)
qd_dict<-as.list(keys)
names(qd_dict)<-dict[[code]]
qd_dict<-dictionary(qd_dict,tolower = FALSE)

# Identify geo tags (states or reg or org ...)
toks_tags <- tokens_lookup(toks, qd_dict, case_insensitive = F)
toks_tags <- lapply(toks_tags, unique)
toks_tags<-as.tokens(toks_tags)
list_tags<-function(x){res<-paste(x, collapse=' ')}
docvars(qd)[[tagsname]]<-as.character(lapply(toks_tags,FUN=list_tags))
docvars(qd)[[paste("nb",tagsname,sep="")]]<-ntoken(toks_tags)



# Export results
return(qd)
 }
```



## Geographical annotation

### Annotate all entities

In a first step, we annotate all geographic entities together in order to benefit from the cross-definition of their respective compounds. We will separate them by subcategories in a second step. 

```{r annotate, eval=FALSE}

# Less than 5 minutes for the tagging of 3 millions of sentences on a good PC computer.  

t1<-Sys.time()

frcomps<-c("Europe 1", "Atlantic city", "Nantes-Atlantique",
           "Loire-Atlantique", "Pyrénées-Atlantique", "Pyrénées-Atlantiques",
           "Alpes-de-Haute-Provence", "Hautes-Alpes", "Rhône-Alpes","Alpes-Maritimes",
           "Chantiers de l'Atlantique", "TGV Atlantique",
           "Bourse de Paris", "Paris SG", "Ville de Paris", "Grand Paris")

qd <- extract_tags (qd = qd,
                     lang="fr",
                     dict = dict,
                     code = "code",
                     tagsname = "geo",
                     split = c("'","’","-"),
                     comps = frcomps,
                     tolow = FALSE)

t2 = Sys.time()
paste("Program executed in ", t2-t1)

table(qd$nbgeo)





```

### Extract states codes

```{r extract_states}
state<-ent$code[ent$type %in% c("sta","cap")]
test <- paste(state, collapse="|")
x<-as.character(lapply(str_extract_all(qd$geo,paste(test, collapse = '|')), paste,collapse=" "))
x<-gsub("ST_","",x)
x<-gsub("CA_","",x)
y<-tokens(x)
y<-lapply(y, unique)
list_tags<-function(x){res<-paste(x, collapse=' ')}
docvars(qd)[["states"]]<-as.character(lapply(y,FUN=list_tags))
docvars(qd)[["nbstates"]]<-ntoken(qd$states)


```

### check news with maximum state number

```{r check_states_news}
table(qd$nbstates)
check<-corpus_subset(qd,nbstates>7)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),states=check$states,nbstates=check$nbstates)
x<-x[order(x$nbstates,decreasing = T),]
kable(x)
```




### Extract world region codes

We do not distinguish so-called "geographical" regions (like "Europe") and "political" regions (like "European Union") and put them in the same catagory of world regions i.e. first level of organization under the world level and/or first level of agregation above state level.


```{r extract_regions}
region<-ent$code[ent$type %in% c("sea","land","cont","org")]
test <- paste(region, collapse="|")
x<-as.character(lapply(str_extract_all(qd$geo,paste(test, collapse = '|')), paste,collapse=" "))
y<-tokens(x)
y<-lapply(y, unique)
list_tags<-function(x){res<-paste(x, collapse=' ')}
docvars(qd)[["regions"]]<-as.character(lapply(y,FUN=list_tags))
docvars(qd)[["nbregions"]]<-ntoken(qd$regions)
table(qd$nbregions)

```

### Check news with maximum number of world regions

```{r check_regions_news}
table(qd$nbregions)
check<-corpus_subset(qd,nbregions>2)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),regions=check$regions,nbregions=check$nbregions)
x<-x[order(x$nbregions,decreasing = T),]
kable(x)
```

### Check news with mixtures of more than 2 states and more than 1 world regions

```{r check_states_regions_news}

check<-corpus_subset(qd,nbregions>1 & nbstates >2)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),geo=check$geo,nbstates=check$nbstates, nbregions = check$nbregions)
x<-x[order(x$nbstates*x$nbregions,decreasing = T),]
kable(x)
```


### Save geographically anotated corpus

```{r}
saveRDS(qd,"data/corpus/qd_mycorpus_geo.RDS")
paste("Size of resulting file = ",round(file.size("data/corpus/qd_mycorpus_geo.RDS")/1000000,3), "Mo")
```





