---
title: "Geographical analysis of media"
subtitle: "2. Geographical tags"
author: "Claude Grasland"
output: html_notebook
---


```{r setup2, echo = FALSE, comment = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = FALSE, warning = FALSE)
library(knitr)
library(dplyr)
library(quanteda)
library(stringr)
library(data.table)
```




# Geographical tags

The aim of this section is to add to the quanteda corpus different metadata related to the geographical entities that are mentioned in the news. We do not discuss here the problems related to the choice of a list of entities and we just apply a method of recognition based on a dictionary. It is theoretically possible to recognize a great number of spatial entities (regions, continents, cities, ...) put we will limit here our research to the case of states recognized at UN and some adding territories partly recognized like Kosovo, Northern Cyprus or Taiwan. 


## Preparation of data



### Load dictonary

We start by loading the last version of the Imageun dictionary and we extract our target language (here : french).

```{r load_dict}
# Load multilanguage dictionary
dict<-fread("dict/global_state_V2.csv")

# Select french dictionary
dict <- dict[dict$lang=="fr",]


head(dict)
```

### Load corpus

```{r}
qd <- readRDS("data/corpus/qd_mycorpus.RDS")
```

### Load tagging function

```{r func_annotate}
extract_tags <- function(qd = qd,                      # the corpus of interest
                         lang = "fr",                  # the language to be used
                         dict = dict,                  # the dictionary of target 
                         code = "ISO3" ,                # variable used for coding
                         alias = "x",                   # variable used for alias
                         tagsname = "states",           # name of the tags column
                         split  = c("'","’","-"),       # split list
                         tolow = TRUE  ,                # Tokenize text
                         comps = c("Afrique du sud")  # compounds
                         )
{ 


  
# Tokenize  
x<-as.character(qd)


if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       x <- gsub(reg," ",x)}  
if(tolow) { x <- tolower(x)} 
toks<-tokens(x)

# compounds
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       comps<- gsub(reg," ",comps)}  
if(tolow)       {comps <- tolower(comps)}  
toks<-tokens_compound(toks,pattern=phrase(comps))

  
# Load dictionaries and create compounds

  ## Target dictionary

labels <-dict[[alias]]
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       labels<- gsub(reg," ",labels)}  
if(tolow)       {labels <- tolower(labels)}  
toks<-tokens_compound(toks,pattern=phrase(labels))
  
 # create quanteda dictionary
keys <-gsub(" ","_",labels)
qd_dict<-as.list(keys)
names(qd_dict)<-dict[[code]]
qd_dict<-dictionary(qd_dict,tolower = FALSE)

# Identify geo tags (states or reg or org ...)
toks_tags <- tokens_lookup(toks, qd_dict, case_insensitive = F)
toks_tags <- lapply(toks_tags, unique)
toks_tags<-as.tokens(toks_tags)
list_tags<-function(x){res<-paste(x, collapse=' ')}
docvars(qd)[[tagsname]]<-as.character(lapply(toks_tags,FUN=list_tags))
docvars(qd)[[paste("nb",tagsname,sep="")]]<-ntoken(toks_tags)



# Export results
return(qd)
 }
```



## Geographical annotation

### Annotate all entities

In a first step, we annotate all geographic entities together in order to benefit from the cross-definition of their respective compounds. We will separate them by subcategories in a second step. 

```{r annotate, eval=FALSE}



t1<-Sys.time()

frcomps<-c("mer de Chine", "océan indien", "continent américain", "pays américains")

qd <- extract_tags (qd = qd,
                     lang="fr",
                     dict = dict,
                     code = "ISO3",
                     alias = "x",
                     tagsname = "states",
                     split = c("'","’","-"),
                     comps = frcomps,
                     tolow = TRUE)

t2 = Sys.time()
paste("Program executed in ", t2-t1)

table(qd$nbstates)





```



### check news with maximum state number

```{r check_states_news}
table(qd$nbstates)
check<-corpus_subset(qd,nbstates>9)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),states=check$states,nbstates=check$nbstates)
x<-x[order(x$nbstates,decreasing = T),]
kable(x)
```







### Save geographically anotated corpus

```{r}
saveRDS(qd,"data/corpus/qd_mycorpus_states.RDS")
paste("Size of resulting file = ",round(file.size("data/corpus/qd_mycorpus_states.RDS")/1000000,3), "Mo")
```





